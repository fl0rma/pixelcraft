{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0833ef14-20e2-47b0-9dd0-e206494ed2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import csv\n",
    "import math\n",
    "import pandas as pd\n",
    "import time "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4cfc65-54f6-4cd2-a33c-8bca6f1415f2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1. Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83d2f5be-f23c-4e08-892a-77ccad76e6e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What type of pattern do you want?\n",
      "Option 1: Lego Pattern\n",
      "Option 2: Cross Stich Pattern\n"
     ]
    }
   ],
   "source": [
    "#inputs from user\n",
    "\n",
    "print(\"What type of pattern do you want?\")\n",
    "print(\"Option 1: Lego Pattern\")\n",
    "print(\"Option 2: Cross Stich Pattern\")\n",
    "\n",
    "choice = input(\"Enter the number of your choice: \")\n",
    "\n",
    "if choice == \"1\":\n",
    "    desired_pattern = \"lego\"\n",
    "elif choice == \"2\":\n",
    "    desired_pattern = \"cross_stich\"\n",
    "else:\n",
    "    print(\"Invalid choice. Please enter a valid option (1 or 2).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4e50bbc-34b4-4e52-9542-b9dfee8edc81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many pieces, each conforming to the width of the desired image, will there be?\n"
     ]
    }
   ],
   "source": [
    "#inputs from user\n",
    "while True:\n",
    "    if desired_pattern == \"lego\":\n",
    "        print(\"How many pieces, each conforming to the width of the desired image, will there be?\")\n",
    "        count = input(\"Enter the number: \")\n",
    "        \n",
    "        # Attempt to convert the input to an integer\n",
    "        try:\n",
    "            count = int(count)\n",
    "            break  # Exit the loop if conversion to int is successful\n",
    "        except ValueError:\n",
    "            print(\"Invalid input. Please enter a valid integer.\")\n",
    "    else:\n",
    "        print(\"How many stitches, each conforming to the width of the desired image, will there be?\")\n",
    "        count = input(\"Enter the number: \")\n",
    "        \n",
    "        # Attempt to convert the input to an integer\n",
    "        try:\n",
    "            count = int(count)\n",
    "            break  # Exit the loop if conversion to int is successful\n",
    "        except ValueError:\n",
    "            print(\"Invalid input. Please enter a valid integer.\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a339aaa-7d8c-46d1-bbd2-51f06a53374d",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_var = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584b9c15-53d5-41c0-92b7-18b741e09e89",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2. Cartoon Effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b7aa383-9752-447c-805c-cc7a196aeaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_image_path = './Pikachu.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c5c4745-ce2e-489e-84ca-79953c3773e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def read_file(filename):\n",
    "#     img = cv2.imread(filename)\n",
    "#     cv2_imshow(img)\n",
    "#     return img\n",
    "# BORRAR\n",
    "img = cv2.imread(input_image_path)  # Read the image from the specified path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e83098a1-f4ca-4e6c-99c5-eeebcbe62d64",
   "metadata": {},
   "outputs": [],
   "source": [
    " #Detect the edge in an image by using the cv2.adaptiveThreshold() function for a cartoon effect.\n",
    "def edge_mask(img, line_size, blur_value):\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    gray_blur = cv2.medianBlur(gray, blur_value)\n",
    "    edges = cv2.adaptiveThreshold(gray_blur, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, line_size, blur_value)\n",
    "    return edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e54a73f6-36bd-4a9e-b513-0b9c12fd8ce7",
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.5.0) ../modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m/Users/florma/projects/pixelcraft/jupyter_test/model_testing.ipynb Cell 10\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/florma/projects/pixelcraft/jupyter_test/model_testing.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m line_size \u001b[39m=\u001b[39m \u001b[39m7\u001b[39m     \u001b[39m#A larger line size means the thicker edges that will be emphasized in the image\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/florma/projects/pixelcraft/jupyter_test/model_testing.ipynb#X12sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m blur_value \u001b[39m=\u001b[39m \u001b[39m7\u001b[39m    \u001b[39m#The larger blur value means fewer black noises appear in the image\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/florma/projects/pixelcraft/jupyter_test/model_testing.ipynb#X12sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m edges \u001b[39m=\u001b[39m edge_mask(img, line_size, blur_value)\n",
      "\u001b[1;32m/Users/florma/projects/pixelcraft/jupyter_test/model_testing.ipynb Cell 10\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/florma/projects/pixelcraft/jupyter_test/model_testing.ipynb#X12sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39medge_mask\u001b[39m(img, line_size, blur_value):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/florma/projects/pixelcraft/jupyter_test/model_testing.ipynb#X12sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m    gray \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39;49mcvtColor(img, cv2\u001b[39m.\u001b[39;49mCOLOR_BGR2GRAY)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/florma/projects/pixelcraft/jupyter_test/model_testing.ipynb#X12sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m    gray_blur \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mmedianBlur(gray, blur_value)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/florma/projects/pixelcraft/jupyter_test/model_testing.ipynb#X12sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m    edges \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39madaptiveThreshold(gray_blur, \u001b[39m255\u001b[39m, cv2\u001b[39m.\u001b[39mADAPTIVE_THRESH_MEAN_C, cv2\u001b[39m.\u001b[39mTHRESH_BINARY, line_size, blur_value)\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.5.0) ../modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n"
     ]
    }
   ],
   "source": [
    "line_size = 7     #A larger line size means the thicker edges that will be emphasized in the image\n",
    "blur_value = 7    #The larger blur value means fewer black noises appear in the image\n",
    "\n",
    "edges = edge_mask(img, line_size, blur_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9172c6e-25de-472d-850f-94c6218e8c4c",
   "metadata": {},
   "source": [
    "##### Color Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ed0682a-f337-4375-9cc5-dd76e10e64fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_quantization(img, k):\n",
    "    # Transform the image into a NumPy array of floating-point numbers, containing the RGB codes \n",
    "    data = np.float32(img).reshape((-1, 3))\n",
    "\n",
    "    # Determine criteria: 20 max iterations, 0.001 epsilon value for level of error\n",
    "    criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 20, 0.001)\n",
    "\n",
    "    # Implementing K-Means (None for initial cluster centers, it will start randomly / 10 is the number of times it will run and it will use the best of those) \n",
    "    ret, label, center = cv2.kmeans(data, k, None, criteria, 5, cv2.KMEANS_RANDOM_CENTERS)\n",
    "\n",
    "    # Cluster centers are converted to unsigned 8-bit integers (uint8) to make them suitable for representing colors in images\n",
    "    center = np.uint8(center)\n",
    "\n",
    "    # Assigns each pixel in the image to its nearest cluster center, effectively recoloring the image\n",
    "    result = center[label.flatten()]\n",
    "\n",
    "    # Result array is converted back to the original shape of the input image (img)\n",
    "    result = result.reshape(img.shape)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da68985b-ece8-4ee4-9efb-1a2459aad126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply color quantization \n",
    "total_color = 10\n",
    "img = color_quantization(img, total_color)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec383381-2386-483a-a503-d3e21a9f9296",
   "metadata": {},
   "source": [
    "##### Bilateral Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a31b42d-17fd-411b-b634-5431f9cb1307",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can reduce the noise in the image by using a bilateral filter. It would give a bit blurred and sharpness-reducing effect to the image.\n",
    "d = 7            #Diameter of each pixel neighborhood\n",
    "sigmaColor=200   #A larger value of the parameter means larger areas of semi-equal color.\n",
    "sigmaSpace=200  #A larger value of the parameter means that farther pixels will influence each other as long as their colors are close enough.\n",
    "\n",
    "blurred = cv2.bilateralFilter(img, d, sigmaColor,sigmaSpace)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ce71c3-4718-443b-b21e-2492fae4730f",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Combine Edge Mask with the Colored Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "08adee20-651f-4e2c-8782-f3653daf5710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining the edge mask that we created earlier, with the color-processed image.\n",
    "cartoon = cv2.bitwise_and(blurred, blurred, mask=edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3cc69762-51e0-472f-be19-79600a6d6fc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.imwrite('cartoon.png', cartoon)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c58c02c-4e12-4f53-8223-6606031e217b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3. Pattern processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "80a2d304-417c-47ca-9911-7b1530789908",
   "metadata": {},
   "outputs": [],
   "source": [
    "if count <= 60:\n",
    "    new_width = 1000\n",
    "elif count >60 & count <= 120:\n",
    "    new_width = 1500\n",
    "elif count >120 & count <= 180:\n",
    "    new_width = 2000\n",
    "elif count >180 & count <= 240:\n",
    "    new_width = 2500\n",
    "elif count >240:\n",
    "    new_width = 3000\n",
    "\n",
    "pixel_size = int(new_width / int(count))\n",
    "\n",
    "# Get the current height and width of the image\n",
    "height, width, _ = cartoon.shape\n",
    "\n",
    "# Calculate new height while maintaining aspect ratio\n",
    "new_height = int(new_width * height / width)\n",
    "\n",
    "# Resize the image using OpenCV\n",
    "resized_img = cv2.resize(cartoon, (new_width, new_height), interpolation=cv2.INTER_NEAREST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2679237d-5df6-4696-af9d-f0925200e463",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pixelate(image_array, pixel_size):\n",
    "    # Extracts the height and width using the shape attribute\n",
    "    img_height, img_width, _ = image_array.shape\n",
    "    \n",
    "    # Calculates the number of steps or blocks along the x and y dimensions of the image based on the pixel_size. \n",
    "    x_steps = img_width // pixel_size\n",
    "    y_steps = img_height // pixel_size\n",
    "    \n",
    "    # This array will be used to store the pixelated version of the image.\n",
    "    pixelated_image = np.zeros_like(image_array)\n",
    "\n",
    "    for y in range(y_steps):          # This outer loop iterates over the vertical blocks (rows) of the image.\n",
    "        for x in range(x_steps):      # This inner loop iterates over the horizontal blocks (columns) of the image.\n",
    "            # This block is essentially a rectangular region of the image, and its size is determined by the pixel_size variable\n",
    "            block = image_array[y * pixel_size : (y + 1) * pixel_size,\n",
    "                                x * pixel_size : (x + 1) * pixel_size]\n",
    "            # Within each block, the code calculates the average color.\n",
    "            average_color = np.mean(np.mean(block, axis=0), axis=0)\n",
    "            # The average color calculated for each block is assigned to all the pixels within that block in the pixelated_image.\n",
    "            pixelated_image[y * pixel_size : (y + 1) * pixel_size,\n",
    "                            x * pixel_size : (x + 1) * pixel_size] = average_color\n",
    "\n",
    "    return pixelated_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f8f5500e-5b43-4871-894c-e988bed005f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the pixelate function with the calculated pixel size\n",
    "pixelated_image = pixelate(resized_img, pixel_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fc724c24-04f8-43d9-b141-2c220bc09fab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.imwrite('pixelated_image.png', pixelated_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "519778bc-1dda-4a05-af5f-aaec4e523499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specific list of RGB colors, representing the available color options for the 1X1 Plate piece at the Lego store and closest thread color\n",
    "lego_data = pd.read_csv('./rgb_lego_colors.csv')\n",
    "thread_data = pd.read_csv('./rgb_threads_colors.csv', encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8923fb8d-17d0-4b42-ba37-9483070d9b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert DataFrame to a list of RGB tuples\n",
    "lego_rgb_values = [(r, g, b) for r, g, b in zip(lego_data['R'], lego_data['G'], lego_data['B'])]\n",
    "thread_rgb_values = [(r, g, b) for r, g, b in zip(thread_data['R'], thread_data['G'], thread_data['B'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "379ad5cd-6d45-4871-92c2-0da0d09ebe35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_quantization2(img, colors):\n",
    "    data = np.float32(img).reshape((-1, 3))\n",
    "    \n",
    "    #Diference respect to color_quantization is that this starts with the specific colors indicated as the centers of the clusters\n",
    "    centers = np.array(colors, dtype=np.float32)\n",
    "    \n",
    "    criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 20, 0.001)\n",
    "    _, label, center = cv2.kmeans(data, len(colors), None, criteria, 10, cv2.KMEANS_USE_INITIAL_LABELS, centers)\n",
    "    center = np.uint8(center)\n",
    "    result = center[label.flatten()]\n",
    "    result = result.reshape(img.shape)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "96b4b088-0f6e-41f1-9acf-de28e8d8b350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply color quantization with the specific colors, to ensure the final result has the avaliable colors\n",
    "if desired_pattern == \"lego\":\n",
    "    specific_colors = lego_rgb_values\n",
    "elif desired_pattern == \"cross_stich\":\n",
    "    specific_colors = thread_rgb_values\n",
    "\n",
    "specific_colors =[(160,165,169),(108,110,104),(5,19,29),(255,255,255)]\n",
    "\n",
    "final_image = color_quantization2(pixelated_image, specific_colors)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8b739add-b3a1-45c7-8d04-e3ea1bac8b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the image to the BGR color space\n",
    "bgr_image = cv2.cvtColor(final_image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "# Create an empty dictionary to store square colors and counts\n",
    "square_colors = {tuple(color): 0 for color in specific_colors}\n",
    "\n",
    "# Function to find the closest color in the specific_colors list\n",
    "def find_closest_color(color):\n",
    "    color = np.array(color)\n",
    "    distances = np.linalg.norm(color - specific_colors, axis=1)\n",
    "    closest_color_index = np.argmin(distances)\n",
    "    return tuple(specific_colors[closest_color_index])\n",
    "\n",
    "# Determine the size of each square based on the gridline spacing\n",
    "square_size = pixel_size\n",
    "\n",
    "# Create a copy of the image to modify\n",
    "result_image = bgr_image.copy()\n",
    "\n",
    "# Create a dictionary to store color-to-symbol mappings\n",
    "color_to_symbol = {}\n",
    "\n",
    "# Generate a list of unique symbols\n",
    "unique_symbols = list(range(1, 39+1))  \n",
    "\n",
    "# Iterate through the unique colors and assign a symbol to each\n",
    "for i, (color, _) in enumerate(square_colors.items()):\n",
    "    symbol = unique_symbols[i]  # Get the next symbol from the list\n",
    "    color_to_symbol[color] = symbol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a8c9558e-e17d-4daf-98de-5bbbb76463e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use color_to_symbol to map colors to symbols in result_image\n",
    "\n",
    "for y in range(0, bgr_image.shape[0], square_size):\n",
    "    for x in range(0, bgr_image.shape[1], square_size):\n",
    "        square = bgr_image[y:y+square_size, x:x+square_size]\n",
    "        square_color = tuple(np.mean(square, axis=(0, 1), dtype=int))\n",
    "        closest_color = find_closest_color(square_color)\n",
    "        \n",
    "        # Get the symbol for the closest color from the dictionary\n",
    "        symbol = color_to_symbol.get(closest_color, 'N/A')  # Add a default value for debugging\n",
    "              \n",
    "        # Increment the count of the closest color in square_colors\n",
    "        square_colors[closest_color] += 1\n",
    "        \n",
    "        # Replace the colors in the square with the closest color\n",
    "        result_image[y:y+square_size, x:x+square_size] = closest_color\n",
    "        \n",
    "        # Overlay the symbol on top of the square\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        font_scale = (pixel_size / 50)\n",
    "        font_color = (0, 0, 0)  # Black font color\n",
    "        font_thickness = 1\n",
    "              \n",
    "        # Calculate the text size to determine the width and height of the text\n",
    "        (text_width, text_height), _ = cv2.getTextSize(str(symbol), font, font_scale, font_thickness)\n",
    "\n",
    "        # Calculate the position (org) to center the text within the square\n",
    "        x_centered = x + (square_size - text_width) // 2\n",
    "        y_centered = y + (square_size + text_height) // 2\n",
    "        org = (x_centered, y_centered)\n",
    "\n",
    "        # Overlay the symbol on top of the square\n",
    "        cv2.putText(result_image, str(symbol), org, font, font_scale, font_color, font_thickness)\n",
    "\n",
    "\n",
    "\n",
    "# Convert the modified image back to RGB color space\n",
    "result_image_rgb = cv2.cvtColor(result_image, cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d2b42a8e-8345-45af-a0f5-4eacba67eb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the gridline spacing based on pixel size\n",
    "gridline_spacing = pixel_size\n",
    "\n",
    "# Create a copy of the pixelated image to draw gridlines on\n",
    "image_with_gridlines = result_image_rgb.copy()\n",
    "\n",
    "# Draw vertical gridlines -  iterates over the coordinates of the image and draws black lines \n",
    "for x in range(0, pixelated_image.shape[1], gridline_spacing):\n",
    "    cv2.line(image_with_gridlines, (x, 0), (x, pixelated_image.shape[0]), (0, 0, 0), 1)  \n",
    "\n",
    "# Draw horizontal gridlines  -  iterates over the coordinates of the image and draws black lines \n",
    "for y in range(0, pixelated_image.shape[0], gridline_spacing):\n",
    "    cv2.line(image_with_gridlines, (0, y), (pixelated_image.shape[1], y), (0, 0, 0), 1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1d38c829-06ae-4c5b-bc7a-da28b0421425",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.imwrite('image_with_gridlines.png', image_with_gridlines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4055ebe-7a66-4e66-b124-e410fdbecc95",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 4. Prepare legend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "44ce7c93-28bc-44e4-aec7-e8fdc8924df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the dictionary to a DataFrame\n",
    "colors_total = pd.DataFrame(list(square_colors.items()), columns=[\"color\", \"count\"])\n",
    "symbols_total = pd.DataFrame(list(color_to_symbol.items()), columns=[\"color\", \"symbol\"])\n",
    "\n",
    "colors_used = pd.merge(colors_total, symbols_total, on='color', how='left')\n",
    "\n",
    "# Drop rows where Count is zero\n",
    "colors_used = (colors_used[colors_used[\"count\"] != 0]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "61cd8231-a1d6-4dde-be61-4ac06ab7eabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_color(x):\n",
    "    return tuple(map(int, str(x).replace(' ', '').strip('()').split(',')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "79a39580-0ddf-45a1-b194-e66eff4996e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors_used['color'] = colors_used['color'].apply(clean_color)\n",
    "lego_data['color'] = lego_data['color'].apply(clean_color)\n",
    "thread_data['color'] = thread_data['color'].apply(clean_color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "28b2c14f-d080-40db-b804-04714fc9c3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if desired_pattern == \"lego\":\n",
    "    color_table = pd.merge(colors_used, lego_data, left_on='color', right_on = 'color', how='left')\n",
    "elif desired_pattern == \"cross_stich\":\n",
    "    color_table = pd.merge(colors_used, thread_data, left_on='color', right_on = 'color', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6bd954e2-b8c1-4f5c-b8f6-e1fad92b4522",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['color','R', 'G','B',]\n",
    "color_table.drop(columns_to_drop, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b4e58f50-af7f-46d8-8450-a74ed408a381",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>symbol</th>\n",
       "      <th>LEGO Name</th>\n",
       "      <th>LEGO Name 2</th>\n",
       "      <th>Element ID</th>\n",
       "      <th>Design ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3782</td>\n",
       "      <td>2</td>\n",
       "      <td>Dark Bluish Gray</td>\n",
       "      <td>199 ['Dark stone grey', 'DK. ST. GREY']</td>\n",
       "      <td>4210719</td>\n",
       "      <td>3024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>124</td>\n",
       "      <td>3</td>\n",
       "      <td>Black</td>\n",
       "      <td>26 ['Black', 'BLACK']\\n342 ['CONDUCT. BLACK']</td>\n",
       "      <td>302426</td>\n",
       "      <td>3024</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count  symbol         LEGO Name   \n",
       "0   3782       2  Dark Bluish Gray  \\\n",
       "1    124       3             Black   \n",
       "\n",
       "                                     LEGO Name 2  Element ID    Design ID  \n",
       "0        199 ['Dark stone grey', 'DK. ST. GREY']      4210719        3024  \n",
       "1  26 ['Black', 'BLACK']\\n342 ['CONDUCT. BLACK']       302426        3024  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "color_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3c2d8e6c-2ade-4999-a903-5de3e2b0f6ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3906"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "color_table['count'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "01b389fd-c018-4129-a7c2-d3d14c2eb06d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3045861164728803\n"
     ]
    }
   ],
   "source": [
    "check = (time.time() - time_var)/60\n",
    "print(check)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
